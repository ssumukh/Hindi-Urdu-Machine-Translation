{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumukh.s/anaconda2/envs/nlpProj/lib/python3.6/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "files = os.listdir(data_folder)\n",
    "source_files = [os.path.join(data_folder, file) for file in files if \".ur\" in file]\n",
    "target_files = [os.path.join(data_folder, file) for file in files if \".hn\" in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/train_source.ur', 'data/test_source.ur']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/train_target.hn', 'data/test_target.hn']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(filenames):\n",
    "    vocab = set()\n",
    "    no_words = 0\n",
    "    for file in filenames:\n",
    "        content = open(file).read()\n",
    "        sentences = [sentence.split() for sentence in content.split('\\n')]\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab.add(word.lower())\n",
    "                    no_words += 1\n",
    "    vocab = {word: num for num, word in enumerate(vocab)}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files_to_indices(filename, vocab):\n",
    "    content = open(filename).read()\n",
    "    return [[vocab[word] for word in sentence.split() if word in vocab] for sentence in content.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveVocabToFl(vocabSet):\n",
    "    a=list(vocabSet)\n",
    "    with open('vocabSv.txt', 'w+') as filehandle:  \n",
    "        for listitem in a:\n",
    "            filehandle.write('%s\\n' % listitem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSrcVcb():\n",
    "    vcb = []\n",
    "    # open file and read the content in a list\n",
    "    with open('vocabSv.txt', 'r') as filehandle:  \n",
    "        for line in filehandle:\n",
    "            # remove linebreak which is the last character of the string\n",
    "            iii = line[:-1]\n",
    "\n",
    "            # add item to the list\n",
    "            vcb.append(iii)\n",
    "    return set(vcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4', '1', '5', '3', '2'}\n"
     ]
    }
   ],
   "source": [
    "a={2,3,1,4,5,1,2,1,2,1}\n",
    "saveVocabToFl(a)\n",
    "print(getSrcVcb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, source_files, target_files, batch_size=10):\n",
    "        self.source_vocab = generate_vocab(source_files)\n",
    "        saveVocabToFl(self.source_vocab)\n",
    "        self.target_vocab = generate_vocab(target_files)\n",
    "        \n",
    "        self.len_source = len(self.source_vocab.keys())\n",
    "        self.source_pad, self.source_start, self.source_end = self.len_source + 2, self.len_source + 1, self.len_source\n",
    "        self.len_target = len(self.target_vocab.keys())\n",
    "        self.target_pad, self.target_start, self.target_end = self.len_target + 2, self.len_target + 1, self.len_target\n",
    "        \n",
    "        self.source_vocab['<pad>'], self.source_vocab['<start>'], self.source_vocab['<end>'] = [self.source_pad, \n",
    "                                                                                                self.source_start, \n",
    "                                                                                                self.source_end]\n",
    "        self.target_vocab['<pad>'], self.target_vocab['<start>'], self.target_vocab['<end>'] = [self.target_pad,\n",
    "                                                                                                self.target_start, \n",
    "                                                                                                self.target_end]\n",
    "        self.len_source = len(self.source_vocab.keys())\n",
    "        self.len_target = len(self.target_vocab.keys())\n",
    "        \n",
    "        self.source_vocab_inv = {value:key for key, value in self.source_vocab.items()}\n",
    "        self.target_vocab_inv = {value:key for key, value in self.target_vocab.items()}\n",
    "\n",
    "\n",
    "        for filename in source_files:\n",
    "            if 'train' in filename:\n",
    "                self.source_train = parse_files_to_indices(filename, self.source_vocab)\n",
    "            if 'test' in filename:\n",
    "                self.source_test = parse_files_to_indices(filename, self.source_vocab)\n",
    "                \n",
    "        for filename in target_files:\n",
    "            if 'train' in filename:\n",
    "                self.target_train = parse_files_to_indices(filename, self.target_vocab)\n",
    "            if 'test' in filename:\n",
    "                self.target_test = parse_files_to_indices(filename, self.target_vocab)\n",
    "                \n",
    "            \n",
    "        self.indices = list(range(len(self.source_train)))\n",
    "        random.shuffle(self.indices)\n",
    "        self.current = -batch_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __get_batch_input(self, indices):\n",
    "        source, target_input, target_target = [], [], []\n",
    "        \n",
    "        # padding length\n",
    "        source_max_len = max(len(self.source_train[i]) for i in indices)\n",
    "        target_max_len = max(len(self.target_train[i]) for i in indices)\n",
    "        \n",
    "        for i in indices:\n",
    "            length = len(self.source_train[i])\n",
    "            # reverse_source_sentences and pad at beginning\n",
    "            sentence = [self.source_pad for _ in range(source_max_len - length)] + [self.source_end] + self.source_train[i][::-1] + [self.source_start]\n",
    "            source.append(sentence)\n",
    "            \n",
    "            length = len(self.target_train[i])\n",
    "            # padding at end for target\n",
    "            sentence = [self.target_start] + self.target_train[i] + [self.target_end] + [self.target_pad for _ in range(target_max_len - length)]\n",
    "            target_input.append(sentence)\n",
    "            target_target.append(sentence[1:] + [self.target_pad])\n",
    "            \n",
    "        return np.array(source), np.array(target_input), np.array(target_target)\n",
    "    \n",
    "    def get_batch_input(self):\n",
    "        if self.current > len(self.indices) - self.batch_size:\n",
    "            self.current = 0\n",
    "            return None, None, None\n",
    "        self.current += self.batch_size\n",
    "        return self.__get_batch_input(self.indices[self.current: self.current + self.batch_size])\n",
    "    \n",
    "    def convert_indices_to_words(self, indices):\n",
    "        output = []\n",
    "        for sentence in indices:\n",
    "            s = []\n",
    "            for word in sentence:\n",
    "                s.append(self.target_vocab_inv[word])\n",
    "            output.append(s)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%run seq2seq.ipynb\n",
    "\n",
    "d = Dataset(source_files, target_files, 10)\n",
    "m = Seq2Seq(d.len_source, d.len_target, 9, 7)\n",
    "m.cuda()\n",
    "\n",
    "a, b = d.get_batch_input()\n",
    "source, target = torch.cuda.LongTensor(a), torch.cuda.LongTensor(b)\n",
    "\n",
    "# For teacher forcing:\n",
    "m(source, target)\n",
    "\n",
    "# Using model's own predictions:\n",
    "m(source)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, coverage=False, coverage_type=\"linguistic\", iterations=1, use_teacher_forcing=True, log=True):\n",
    "    # TODO: attention error\n",
    "    loss_func = nn.NLLLoss()\n",
    "    attn_loss_func = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adagrad(model.parameters())\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        while True:\n",
    "            optimizer.zero_grad()\n",
    "            source, target_input, target_output = d.get_batch_input()\n",
    "\n",
    "            if source is None: # end of current iteration\n",
    "                break\n",
    "\n",
    "            source, target_input, target_output = [torch.cuda.LongTensor(source), \n",
    "                                                   torch.cuda.LongTensor(target_input), \n",
    "                                                   torch.cuda.LongTensor(target_output)]\n",
    "            \n",
    "            source_mask = torch.ones(source.shape).cuda()\n",
    "            source_mask[source == dataset.source_pad] = 0\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                pred, attn = model(source, target_input, source_mask=source_mask)\n",
    "                # mask whatevers after <stop> \n",
    "                target_mask = torch.ones(target_output.shape).cuda()\n",
    "                target_mask[target_output == dataset.target_pad] = 0\n",
    "                pred = pred * target_mask.unsqueeze(-1)\n",
    "                target_output = target_output * target_mask.long()\n",
    "            else:\n",
    "                pred, attn_weights = model(source)\n",
    "                \n",
    "            no_words = pred.shape[0] * pred.shape[1]\n",
    "            pred = pred.reshape(no_words, -1)\n",
    "            target_output = target_output.reshape(no_words)\n",
    "\n",
    "            pred_error = loss_func(pred, target_output)\n",
    "            attn_error = None\n",
    "            \n",
    "            if coverage:\n",
    "                # if coverage type is linguistic, ignore fertility\n",
    "                attn_weights, fertility = attn\n",
    "                if coverage_type == \"linguistic\":\n",
    "                    fertility = torch.ones(fertility.shape).cuda()\n",
    "                attn_error = attn_loss_func(torch.sum(attn_weights, dim=-1) * source_mask, fertility * source_mask)\n",
    "                pred_error += attn_error\n",
    "                \n",
    "            pred_error.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if log:\n",
    "                print(d.current/d.batch_size, pred_error, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pred_to_indices(pred):\n",
    "    return torch.max(pred, dim=-1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset, log=False, source_test=None, target_test=None):\n",
    "    scores = []\n",
    "    if source_test is None:\n",
    "        source_test = dataset.source_test\n",
    "        target_test = dataset.target_test\n",
    "        \n",
    "    for sentence, target in zip(source_test, target_test):\n",
    "        input = torch.cuda.LongTensor([[dataset.source_end] + sentence[::-1] + [dataset.source_start]])\n",
    "        pred, _ = model(input)\n",
    "        pred_words = convert_pred_to_indices(pred).cpu().numpy()\n",
    "        predicted_target = dataset.convert_indices_to_words(pred_words)\n",
    "        target = dataset.convert_indices_to_words([target])\n",
    "        if log:\n",
    "            print(target, predicted_target[0])\n",
    "        scores.append(sentence_bleu(target, predicted_target[0], weights=(1, 0, 0, 0)))\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
